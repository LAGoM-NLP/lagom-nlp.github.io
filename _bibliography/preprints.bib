@misc{ploeger2024principled,
  title         = {A {{Principled Framework}} for {{Evaluating}} on {{Typologically Diverse Languages}}},
  author        = {Ploeger, Esther and Poelman, Wessel and {H{\o}eg-Petersen}, Andreas Holck and Schlichtkrull, Anders and {de Lhoneux}, Miryam and Bjerva, Johannes},
  year          = {2024},
  month         = jul,
  number        = {arXiv:2407.05022},
  eprint        = {2407.05022},
  url           = {http://arxiv.org/abs/2407.05022},
  urldate       = {2024-07-22},
  abstract      = {Beyond individual languages, multilingual natural language processing (NLP) research increasingly aims to develop models that perform well across languages generally. However, evaluating these systems on all the world's languages is practically infeasible. To attain generalizability, representative language sampling is essential. Previous work argues that generalizable multilingual evaluation sets should contain languages with diverse typological properties. However, 'typologically diverse' language samples have been found to vary considerably in this regard, and popular sampling methods are flawed and inconsistent. We present a language sampling framework for selecting highly typologically diverse languages given a sampling frame, informed by language typology. We compare sampling methods with a range of metrics and find that our systematic methods consistently retrieve more typologically diverse language selections than previous methods in NLP. Moreover, we provide evidence that this affects generalizability in multilingual model evaluation, emphasizing the importance of diverse language sampling in NLP evaluation.},
  keywords      = {Computer Science - Computation and Language},
  archiveprefix = {arxiv},
  abbr          = {arXiv},
  primaryclass  = {cs},
  publisher     = {arXiv},
  bibtex_show   = true
}

@misc{tatariya2024how,
  title         = {How {{Good}} Is {{Your Wikipedia}}?},
  author        = {Tatariya*, Kushal and Kulmizev*, Artur and Poelman, Wessel and Ploeger, Esther and Bollmann, Marcel and Bjerva, Johannes and Luo, Jiaming and Lent, Heather and de Lhoneux, Miryam},
  year          = {2024},
  month         = nov,
  number        = {arXiv:2411.05527},
  eprint        = {2411.05527},
  publisher     = {arXiv},
  url           = {http://arxiv.org/abs/2411.05527},
  urldate       = {2024-11-13},
  abstract      = {Wikipedia's perceived high quality and broad language coverage have established it as a fundamental resource in multilingual NLP. In the context of low-resource languages, however, these quality assumptions are increasingly being scrutinised. This paper critically examines the data quality of Wikipedia in a non-English setting by subjecting it to various quality filtering techniques, revealing widespread issues such as a high percentage of one-line articles and duplicate articles. We evaluate the downstream impact of quality filtering on Wikipedia and find that data quality pruning is an effective means for resource-efficient training without hurting performance, especially for low-resource languages. Moreover, we advocate for a shift in perspective from seeking a general definition of data quality towards a more language- and task-specific one. Ultimately, we aim for this study to serve as a guide to using Wikipedia for pretraining in a multilingual setting.},
  keywords      = {Computer Science - Computation and Language},
  archiveprefix = {arxiv},
  abbr          = {arXiv},
  primaryclass  = {cs},
  bibtex_show   = true
}
